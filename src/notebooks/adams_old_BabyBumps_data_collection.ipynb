{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [01:44<00:00,  5.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((314472, 41),\n",
       " datetime.datetime(2011, 1, 4, 18, 25, 57),\n",
       " datetime.datetime(2021, 3, 15, 19, 23, 34))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=\"data/subreddits/BabyBumps/submissions/\"\n",
    "orig_df = pd.DataFrame()\n",
    "for entry in tqdm(os.listdir(PATH)):\n",
    "    if not entry.endswith(\".json.gz\"):\n",
    "        continue\n",
    "    orig_df = pd.concat([orig_df, pd.read_json(PATH+entry, compression='infer')], axis=0)\n",
    "orig_df.shape, datetime.utcfromtimestamp(min(orig_df['created_utc'])), datetime.utcfromtimestamp(max(orig_df['created_utc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314472, 41)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df = orig_df.drop_duplicates(['id','created_utc', 'author'],keep='last')\n",
    "orig_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlier_df = pd.DataFrame()\n",
    "# for entry in tqdm(os.listdir(PATH+\"original/\")):\n",
    "#     earlier_df = pd.concat([earlier_df, pd.read_json(PATH+\"original/\"+entry, lines=True, compression='gzip')], axis=0)\n",
    "    \n",
    "# earlier_df.shape, min(earlier_df['created_utc']), max(earlier_df['created_utc']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine examples into one dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys_intersect = list(orig_df.keys().intersection(earlier_df.keys()))\n",
    "# combined_df = pd.concat([earlier_df[keys_intersect], orig_df[keys_intersect]]).reset_index(drop=True)\n",
    "# combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df.drop_duplicates(['id','created_utc', 'author'],keep='last')\n",
    "# combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include only examples with \"birth story\" or \"graduat\" in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    306346\n",
      "True       8126\n",
      "Name: is_birth_story, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8126, 42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['is_birth_story'] = combined_df['title'].map(lambda x: \"birth story\" in x.lower() or 'graduat' in x.lower())\n",
    "print(combined_df['is_birth_story'].value_counts())\n",
    "df = combined_df[combined_df['is_birth_story']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `pre-covid` indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoliak/anaconda3/envs/reddit-retreiver/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     7147\n",
       "False     979\n",
       "Name: pre-covid, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1583020801 - march 1st 2020\n",
    "# 1584244800 - march 15th 2020\n",
    "df['pre-covid'] = df['created_utc'].map(lambda x: True if x < 1584244800 else False) #1583020801 is unix timestamp for March 1st 2020 at 12:00:01 am\n",
    "df['pre-covid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"birth_narratives.jsonl.gz\", lines=True, compression=\"gzip\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_json(\"birth_narratives.jsonl.gz\", lines=True, compression=\"gzip\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6386, 43)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aligning empty stories with corresponding comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>pre-covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i1ula0</td>\n",
       "      <td>GhxstCxt</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i2l1y8</td>\n",
       "      <td>bloop_bloop_bloooooo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i2q6ph</td>\n",
       "      <td>Watchingpornwithcas</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i33b32</td>\n",
       "      <td>DashOfLiz</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i3awn7</td>\n",
       "      <td>fluorescentpuffin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8105</th>\n",
       "      <td>7di89o</td>\n",
       "      <td>Duckyes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8107</th>\n",
       "      <td>112c2k</td>\n",
       "      <td>fillie</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8108</th>\n",
       "      <td>117yty</td>\n",
       "      <td>derpitydooda</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8109</th>\n",
       "      <td>1181qn</td>\n",
       "      <td>chancesofconception</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8111</th>\n",
       "      <td>11arb6</td>\n",
       "      <td>eet</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                author  pre-covid\n",
       "0     i1ula0              GhxstCxt      False\n",
       "3     i2l1y8  bloop_bloop_bloooooo      False\n",
       "4     i2q6ph   Watchingpornwithcas      False\n",
       "7     i33b32             DashOfLiz      False\n",
       "9     i3awn7     fluorescentpuffin      False\n",
       "...      ...                   ...        ...\n",
       "8105  7di89o               Duckyes       True\n",
       "8107  112c2k                fillie       True\n",
       "8108  117yty          derpitydooda       True\n",
       "8109  1181qn   chancesofconception       True\n",
       "8111  11arb6                   eet       True\n",
       "\n",
       "[2651 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_text_df = df[df['selftext'].map(lambda x: not x)]\n",
    "missing_id_author_df = missing_text_df[['id', 'author', 'pre-covid']]\n",
    "missing_id_author_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2077\n",
       "False     574\n",
       "Name: pre-covid, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_id_author_df['pre-covid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoliak/anaconda3/envs/reddit-retreiver/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     1740\n",
       "False     911\n",
       "Name: body, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_first_comment(row):\n",
    "    curr_id, author = row.id, row.author\n",
    "    if not os.path.exists(f\"data/subreddits/BabyBumps/comments/{curr_id}.json.gz\"):\n",
    "        return \n",
    "    comments_df = pd.read_json(f\"data/subreddits/BabyBumps/comments/{curr_id}.json.gz\", compression='gzip')\n",
    "    if comments_df.shape[0] == 0:\n",
    "        return\n",
    "    match_df = comments_df[(comments_df['parent_id'].map(lambda x: curr_id in x)) & (comments_df['author'] == author)].sort_values('created_utc',ascending=True)\n",
    "    if match_df.shape[0] == 0:\n",
    "        return \n",
    "    return match_df.iloc[0]['body']\n",
    "\n",
    "missing_id_author_df['body'] = missing_id_author_df.apply(get_first_comment, axis=1)\n",
    "missing_id_author_df['body'].map(lambda x: x == None).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>pre-covid</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, author, pre-covid, body]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_id_author_df[missing_id_author_df['body'] == None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Add check for pre-covid examples***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update missing selftext's** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    5475\n",
      "True     2651\n",
      "Name: selftext, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    6386\n",
       "True     1740\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df['selftext'].map(lambda x: not x).value_counts())\n",
    "for idx, row in missing_id_author_df.iterrows():\n",
    "    df.at[idx, 'selftext'] = row.body\n",
    "    #if row.body:\n",
    "    #    df.loc[row.index]['selftext'] = row.body\n",
    "df['selftext'].map(lambda x: not x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     6386\n",
       "False    1740\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['selftext'].map(lambda x: x != None).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6386,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['selftext'].map(lambda x: not not x)]['selftext'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6386, 43)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['selftext'].map(lambda x: not not x)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     6317\n",
       "False      69\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['selftext'].map(lambda x: x != '[removed]' or x != '[deleted]').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6119, 43)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['selftext'] != '[removed]']\n",
    "df = df[df['selftext'] != '[deleted]']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"birth_narratives.jsonl.gz\", lines=True, compression=\"gzip\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_tokens'] = df['title'].map(lambda x: nltk.word_tokenize(x.lower()))\n",
    "df['selftext_tokens'] = df['selftext'].map(lambda x: nltk.word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "df['selftext_lemmas'] = df['selftext_tokens'].map(lambda x, lmtzr=lmtzr: [lmtzr.lemmatize(tok) for tok in x])\n",
    "df['title_lemmas'] = df['title_tokens'].map(lambda x, lmtzr=lmtzr: [lmtzr.lemmatize(tok) for tok in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove stories with less than 500 words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(df['selftext_tokens'].map(lambda x: len(x) >= 500).value_counts()))\n",
    "df = df[df['selftext_tokens'].map(lambda x: len(x) >= 500)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(orig_df['created_utc']), max(orig_df['created_utc'])\n",
    "\n",
    "\n",
    "orig_df['timestamp'] = orig_df['created_utc'].map(lambda x: pd.to_datetime(datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S'))) \n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(int(min(df['created_utc']))).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(datetime.utcfromtimestamp(int(max(df['created_utc']))).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "orig_df = orig_df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = df['created_utc'].map(lambda x: pd.to_datetime(datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 5000, 500)\n",
    "bins = np.insert(bins, 1, 1)\n",
    "df['selftext'].map(lambda x: 0 if not x else len(x.split())).hist(bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Length of `stories`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(500, 5000, 100)\n",
    "df['selftext_tokens'].map(lambda x: len(x)).hist(bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantity of Submissions per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['timestamp'].map(lambda x: x.year).value_counts().sort_index().plot(kind=\"bar\", rot=45)\n",
    "#ax.xaxis.set_major_formatter(mdates.DateFormatter())\n",
    "#df['timestamp'].map(lambda x: \"-\".join(x.split(\"-\")[0:2])).value_counts().sort_index().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"babybumps.jsonl.gz\", lines=True, compression='gzip', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test loading dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.read_json(\"babybumps.jsonl.gz\", lines=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.keys() == df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_covid_vc = df[df['pre-covid']]['timestamp'].map(lambda x: pd.to_datetime(f\"{x.year}-{x.month}\")).value_counts().sort_index()\n",
    "day_count_df = pd.DataFrame()\n",
    "day_count_df['ds'] = pre_covid_vc.map(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_count_df['y'] = day_count_df['ds']\n",
    "day_count_df['ds'] = day_count_df.index\n",
    "day_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "m.fit(day_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=365)\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anxious_text_df_long = anxious_text_df[anxious_text_df.selftext.str.split().map(lambda x: len(x) > 99)]\n",
    "anxious_text_df_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(anxious_text_df.selftext_tokens.iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "anxious_text_df_long['tokens'] = anxious_text_df_long.selftext.str.lower().map(lambda x, tokenizer=tokenizer: tokenizer.tokenize(x))\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "anxious_text_df_long['tokens'] = anxious_text_df_long['tokens'].map(lambda x: [token for token in x if not token.isnumeric()])\n",
    "\n",
    "# Remove words that are only one character.\n",
    "anxious_text_df_long['tokens'] = anxious_text_df_long['tokens'].map(lambda x: [token for token in x if len(token) > 1])\n",
    "\n",
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "anxious_text_df_long['tokens'] = anxious_text_df_long['tokens'].map(lambda x: [token for token in x if token not in stop_words])\n",
    "\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "anxious_text_df_long['tokens'] = anxious_text_df_long['tokens'].map(lambda x: [lemmatizer.lemmatize(token) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(anxious_text_df_long['tokens'])\n",
    "\n",
    "\n",
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.10)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "def topics_to_df(top_topics):\n",
    "    topic_id2words = {}\n",
    "    for idx, topic in enumerate(top_topics):\n",
    "        topic_id2words[idx] = []\n",
    "        for word in topic[0][:10]:\n",
    "            topic_id2words[idx].append(word[1])\n",
    "    return pd.DataFrame.from_dict(topic_id2words).T\n",
    "\n",
    "topics_to_df(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-retreiver",
   "language": "python",
   "name": "reddit-retreiver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
